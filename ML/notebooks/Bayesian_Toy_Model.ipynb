{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1fa84ddf92c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import Libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'notebook'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hera3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n",
      "\u001b[0;32m</Users/tashaleebillings/anaconda3/envs/hera3/lib/python3.7/site-packages/decorator.py:decorator-gen-108>\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hera3/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hera3/lib/python3.7/site-packages/IPython/core/magics/pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hera3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hera3/lib/python3.7/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mfind_gui_and_backend\u001b[0;34m(gui, gui_select)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "%matplotlib notebook\n",
    "import numpy as np, matplotlib.pyplot as plt, datetime, tensorflow as tf, os, datetime\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from keras.callbacks import ModelCheckpoint#, TensorBoard                                                                      \n",
    "from keras import backend as K\n",
    "#from tensorboard.plugins.hparams import api as hp                                                                             \n",
    "#from keras.utils.generic_utils import get_custom_objects                                                                      \n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model = np.load('toy_models.npz')\n",
    "nx, ny, ntrain = toy_model['training_data'].shape\n",
    "training_data = toy_model['training_data'].T\n",
    "labels = toy_model['labels']\n",
    "outputdir = \"./\"\n",
    "\n",
    "HP_EPOCH = 15\n",
    "HP_BATCH_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing                                                                                                                  \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 28, 28, 1) / 255.0\n",
    "X_test = X_test.reshape(10000, 28, 28, 1) / 255.0\n",
    "X_train = training_data[0:8000,:,:].reshape(8000,nx,ny,1)\n",
    "y_train = labels[0:8000]*100\n",
    "X_test = training_data[8000:,:,:].reshape(2000,nx,ny,1)\n",
    "y_test = labels[8000:]*100\n",
    "\n",
    "print(\"training\", X_train.shape)\n",
    "print(\"validation\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Loss Functions                                                                                                               \n",
    "kl_divergence = tf.keras.losses.KLDivergence()\n",
    "\n",
    "neg_log_likelihood = lambda y_true, y_pred: -tf.reduce_mean(input_tensor=y_pred.log_prob(y_true))\n",
    "\n",
    "def Mean_Squared_over_true_Error(y_true, y_pred):\n",
    "    # Create a custom loss function that divides the difference by the true                                                           \n",
    "\n",
    "    y_true = K.cast(y_true, y_pred.dtype) #Casts a tensor to a different dtype and returns it.                                        \n",
    "    diff_ratio = K.square((y_pred - y_true)/K.clip(K.abs(y_true),K.epsilon(),None))\n",
    "\n",
    "    loss = K.mean(diff_ratio, axis=-1)\n",
    "\n",
    "    # Return a tensor                                                                                                                 \n",
    "    return loss\n",
    "\n",
    "def elbo(y_true, y_pred):\n",
    "    kl_weight = 1\n",
    "    neg_log_likelihood = -tf.reduce_mean(input_tensor=y_pred.log_prob(y_true))\n",
    "    kl_divergence = tf.keras.losses.KLDivergence()\n",
    "\n",
    "    elbo_loss = -tf.math.reduce_mean(-kl_weight * kl_divergence(y_true, y_pred.mean()) - neg_log_likelihood)\n",
    "    # Return a tensor                                                                                                                 \n",
    "    return elbo_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt TensorFlow runs to log hyperparameters and metrics\n",
    "def train_test_model():\n",
    "    # Build model                                                                                                                                                                                                               \n",
    "    model = Sequential()\n",
    "    # Add layers                                                                                                                                                                                                                \n",
    "    model.add(Conv2D(16, kernel_size=3, activation=\"relu\", input_shape=(nx, ny, 1)))                                                                                                                                            \n",
    "    #model.add(BatchNormalization())                                                                                                                                                                                            \n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))                                                                                                                                                                                  \n",
    "    model.add(Conv2D(8, kernel_size=3, activation=\"relu\"))                                                                                                                                                                      \n",
    "    #model.add(BatchNormalization())                                                                                                                                                                                            \n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))                                                                                                                                                                                  \n",
    "    model.add(Flatten())                                                                                                                                                                                                        \n",
    "    model.add(tfp.layers.DenseFlipout(1, activation=\"linear\"))                                                                                                                                                                  \n",
    "    #model.add(Dense(1,activation=\"linear\"))                                                                                                                                                                                    \n",
    "    model.add(tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1)))                                                                                                                                              \n",
    "                                                                                                                                                                                                                                \n",
    "    model.compile(optimizer= \"adam\", loss = \"mse\")#\"mape\")                                                                                                                                                                      \n",
    "                                                                                                                                                                                                                                \n",
    "    # Visualize Model                                                                                                                                                                                                           \n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test),                                                                                                                                                               \n",
    "                epochs=HP_EPOCH, batch_size=HP_BATCH_SIZE,                                                                                                                                                                      \n",
    "                #callbacks = callbacks_list)                                                                                                                                                                                    \n",
    "               )       \n",
    "    \n",
    "    loss = model.evaluate(X_test, y_test)                                                                                                                                                                                       \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL Model\n",
    "\n",
    "def train_test_model():\n",
    "    # Making a model.                                                                                                                                                                   \n",
    "    model = Sequential()\n",
    "\n",
    "    # Added Layers.                                                                                                                                                                     \n",
    "    model.add(Conv2D(16, kernel_size=3, activation=\"relu\", input_shape=(nx, ny, 1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dense(8, activation = 'relu'))\n",
    "    model.add(Conv2D(8, kernel_size = 3, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(4, activation = 'relu'))\n",
    "    model.add(Conv2D(4, kernel_size = 3, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(2, activation = 'relu'))\n",
    "    model.add(Conv2D(2, kernel_size = 3, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(4, activation = 'relu'))\n",
    "    model.add(Conv2D(4, kernel_size = 3, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(2, activation = 'relu'))\n",
    "    model.add(Conv2D(2, kernel_size = 3, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1, activation = 'relu'))\n",
    "    model.add(Conv2D(1, kernel_size = 3, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    #model.add(tfp.layers.Convolution2DFlipout(1, kernel_size=3, activation = 'relu', padding = 'same'))\n",
    "    model.add(Conv2D(1, kernel_size = 3, activation = 'relu', padding = 'same'))                                                                                                       \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #model.add(Dense(1,activation=\"linear\"))                                                                                                                                            \n",
    "    #model.add(tfp.layers.DenseFlipout(1, activation=\"linear\"))\n",
    "    #model.add(tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1)))                                                                                                     \n",
    "\n",
    "    model.compile(optimizer= \"adam\", loss = \"mape\")#neg_log_likelihood)#\"mse\")#\"mape\")                                                                                                  \n",
    "\n",
    "    # Visualize Model                                                                                                                                                                  \\\n",
    "                                                                                                                                                                                        \n",
    "    print(model.summary())\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                    epochs=HP_EPOCH, batch_size=HP_BATCH_SIZE,\n",
    "                   )\n",
    "\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"                                                                                                                                                                                     \n",
    "# Convert to true tau units                                                                                                                                                             \n",
    "true_tau = low_z_tau + h_2 * result[\"truth\"][:,n]/factor                                                                                                                                \n",
    "predicted_tau = low_z_tau + h_2 * result[\"prediction\"][:,n]/factor                                                                                                                      \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions                                                                                                                                                                      \n",
    "predictions = []\n",
    "for i in range(10):\n",
    "    y_p = model.predict(X_test).squeeze()#predict(X_test, batch_size=test_labels.shape[0])                                                                                              \n",
    "    predictions.append(y_p) # (500, 100, 1) = (# of masks, # of datasets, # of classes)                                                                                                 \n",
    "predictions = np.mean(np.array(predictions), axis=0)\n",
    "print(X_test.shape)\n",
    "print(predictions.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test, predictions, '.')\n",
    "plt.plot(y_test,y_test, \"r--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Layer\n",
    "Goal: (1) Turn the output layer from deterministic to probabilistic **Dense(1,activation=\"linear\") --> DenseFlipout(1, activation=\"linear\")**.  (2) Make one-to-one prediction plots.  (3) Show that the preditioned values form a distibution that behaves apprrroximately Normal.  \n",
    "\n",
    "\n",
    "### Trail 1\n",
    "Loss: MAPE  \n",
    "\n",
    "### Trial 2\n",
    "Loss: MAE  \n",
    "\n",
    "### Trial 3\n",
    "Loss: NLL  \n",
    "*NOTE*: Need to add the LambdaDistribution Layer because when calculating the loss it needs to calculate the negative log of a distribution.  \n",
    "\n",
    "### Trial 4\n",
    "Loss: KL Divergence  \n",
    "\n",
    "### SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENEERAL Model\n",
    "\n",
    "def train_test_model():\n",
    "    # Making a model.                                                                                                                                                                   \n",
    "    model = Sequential()\n",
    "\n",
    "    # Added Layers.                                                                                                                                                                     \n",
    "    model.add(Conv2D(16, kernel_size=3, activation=\"relu\", input_shape=(nx, ny, 1), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dense(8, activation = 'relu'))\n",
    "    model.add(Conv2D(8, kernel_size = 3, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(4, activation = 'relu'))\n",
    "    model.add(Conv2D(4, kernel_size = 3, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(2, activation = 'relu'))\n",
    "    model.add(Conv2D(2, kernel_size = 3, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(4, activation = 'relu'))\n",
    "    model.add(Conv2D(4, kernel_size = 3, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(2, activation = 'relu'))\n",
    "    model.add(Conv2D(2, kernel_size = 3, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1, activation = 'relu'))\n",
    "    model.add(Conv2D(1, kernel_size = 3, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    #model.add(tfp.layers.Convolution2DFlipout(1, kernel_size=3, activation = 'relu', padding = 'same'))\n",
    "    model.add(Conv2D(1, kernel_size = 3, activation = 'relu', padding = 'same'))                                                                                                       \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #model.add(Dense(1,activation=\"linear\"))                                                                                                                                            \n",
    "    model.add(tfp.layers.DenseFlipout(1, activation=\"linear\"))\n",
    "    #model.add(tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1)))                                                                                                     \n",
    "\n",
    "    model.compile(optimizer= \"adam\", loss = \"mape\")#neg_log_likelihood)#\"mse\")#\"mape\")                                                                                                  \n",
    "\n",
    "    # Visualize Model                                                                                                                                                                  \\\n",
    "                                                                                                                                                                                        \n",
    "    print(model.summary())\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                    epochs=HP_EPOCH, batch_size=HP_BATCH_SIZE,\n",
    "                   )\n",
    "\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "    return model\n",
    "\n",
    "model = train_test_model()\n",
    "\n",
    "# make predictions                                                                                                                                                                      \n",
    "predictions = []\n",
    "for i in range(10):\n",
    "    y_p = model.predict(X_test).squeeze()#predict(X_test, batch_size=test_labels.shape[0])                                                                                              \n",
    "    predictions.append(y_p) # (500, 100, 1) = (# of masks, # of datasets, # of classes)                                                                                                 \n",
    "predictions = np.mean(np.array(predictions), axis=0)\n",
    "print(X_test.shape)\n",
    "print(predictions.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Plot one-to-one predictions\n",
    "plt.figure()\n",
    "plt.title(\"\")\n",
    "plt.xlabel(\"Tue Tau\")\n",
    "plt.ylabel(\"Predicted Tau\")\n",
    "plt.plot(y_test, predictions, '.')\n",
    "plt.plot(y_test,y_test, \"r--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
