{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Modeling Techniques\n",
    "\n",
    "Why are we even exploring probabilistic techniques? Many Machine Learning models are trained and tuned using an iterative algorithm designed under a the probabilistic framework. This is because probability theory can be applied to any problem involving uncertainty. Technically you’re using probability and statistics to provide more insight.  \n",
    "\n",
    "In probability theory there are two methods of thinking or inferences, the *Frequentist Inference* and *Bayesian Inference*. Although these are different ways of thinking about the same problem they can converge to the same result. In the limit that you have large amounts of data, Bayesian Inference converges to the Frequentist Inference. In order for a hypothesis (some classifier we train using training data) to have predictive power, Frequentists believe that the parameters that describe your hypothesis are just parameters and you cannot sample points from it because it is not a distribution. It’s just a point. Under Bayesian Inference the parameters that describe your hypothesis are a distribution. Also, you have some prior knowledge oon what you think the distribution of hypothesis should be.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **goal** *of supervised learning algorithms is to estimate the probability distribution from samples*. In general these algorithms can be viewed as estimating $P(X\\cap Y)$ and fall into two categories:\n",
    "\n",
    "- When we estimate $P(X\\cap Y)=P(X|Y)P(Y)$, then we call it generative learning.  \n",
    "- When we only estimate $P(Y|X)$ directly, then we call it discriminative learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment I will initially use for both **MLE** and **MAP** is a simple coin toss. You toss a two sided coin 10 times and we want to know what the probability distribution of this 10-flip sample is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist Inference\n",
    "### Gentle Introduction to [MLE](https://machinelearningmastery.com/what-is-maximum-likelihood-estimation-in-machine-learning/)\n",
    "\n",
    "In Maximum Likelyhood Estimation (**MLE**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference\n",
    "\n",
    "### Gentle Introduction to [MAP](https://machinelearningmastery.com/maximum-a-posteriori-estimation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gentle Introduction to [MCMC](https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image as pil_image\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d5092e34a5e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize_activation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_num_filters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vis'"
     ]
    }
   ],
   "source": [
    "from vis.visualization import visualize_activation, get_num_filters\n",
    "from vis.utils import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
